{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HOML_cap_14.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP/BaMJyGcTx3LGxnJ2eAIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nechebarrena/HOML/blob/main/HOML_cap_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwE6lkc7Lmgn"
      },
      "source": [
        "# **CAPITULO 14 - HANDS ON MACHINE LEARNING**\n",
        "## **Vision por computadora usando CNN's**\n",
        "\n",
        "A lo largo de este capitulo el libro se propone analizar las redes neuronales convolucionales (CNN) y su implementacion con TensorFlow y Keras aplicadas a la vision por computadora. En capitulos posteriores va a utilizar tambien CNN's para otro tipo de tareas.\n",
        "\n",
        "### **Capas convolucionales**\n",
        "Las capas convolucionales son los ladrillos elementales para construir una CNN. \n",
        "Las neuronaes en estas capas no estan conectadas a todos los datos de entrada (que de ahora en mas vamos a suponer que es una matriz donde cada elemento es un pixel y por lo tanto representa una imagen) sino a un pequeño grupo llamado _receptive fields_. Esta forma de conectar las neuronas permite que la red se concentre en atributos de pequeña escala.\n",
        "\n",
        "_ver lo de zero padding_\n",
        "\n",
        "En general los campos de vision o _receptive fields_ entre dos neuronas contiguas se suelen solapar, tal como se muestra en la figura $14.3$ del libro, sin embargo el tamaño del solapamiento puede ser elegido segun la arquitectura. El parametro de control que nos permite seleccionar cuan solapado esta el campo de vision de dos neuronas contiguas se denomina _stride_. Aumentar el stride reduce el solapamiento y esto, obviamente, reduce la cantidad de neuronas en la capa. Esto se ejemplifica en la figura $14.4$ del libro.\n",
        "\n",
        "_Aca el libro no explica demasiado como se conectan ni como son estas neuronas, quizas mas adelante lo explique mejor. Si no lo hace volver aca y explicarlo con alguna otra fuente_\n",
        "\n",
        "### **Filtros**\n",
        "\n",
        "Si pensamos los pesos de una neurona como una matriz que puede tomar distintos valores, poner cero en un elemento implica que no va a considerar el pixel del cual proviene. Por el contrario, poner un uno implica lo contrario, un transpaso perfecto del pixel original. De esta forma, si todas las neuronas en una capa tienen un unico \"filtro\" aplicado a sus pesos entonces la salida de esta imagen sera una nueva imagen donde se resalta las caracteristicas del filtro usado. Ejmeplos tipicos de filtros son los verticales, horizontales, de blurreo, etc. La idea es que uno no seleccione estos filtros sino que sea la red quien aprenda cuales son los filtros ideales dada una funcion de costo y un conjunto de entrenamiento. \n",
        "\n",
        "## **Apilando distintas capas de atributos**\n",
        "\n",
        "Hasta el momento solo utilizamos un \"filtro\" o \"seleccionador de atributo\" por capa, pero de forma mas general uno puede utilizar mas de uno por capa. O sea, puede buscar muchos atributos en una misma capa. Cuando utilizamos un solo filtro por capa lo que estamos haciendo es utilizar una cantidad de $N \\times M$ neuronas, donde este es el numero de pixeles en la capa. Todas estas neuronas comparten los mismos pesos y el mismo bias. Cuando utilizamos mas de un filtro por capa, cada uno de estos filtros comparte los pesos y bias para todas sus neuronas y a su vez estos difieren de los de las neuronas de los otros filtros. Entonces, las neuronas de un filtro en la capa $l$ estan conectadas con todos los features de la capa anterior $l-1$. La mejor forma de pensar esta arquitectura es con tensores de 3 dimensiones. Para esto sirve pensar en la imagen que muestro a continuacion:\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/file/d/1wroiPnT79NFRmfYW3PjWLscX9F9yoaax/preview'/>\n",
        "<figcaption>Image Caption</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<iframe src=\"https://drive.google.com/file/d/1wroiPnT79NFRmfYW3PjWLscX9F9yoaax/preview\" width=\"640\" height=\"480\"></iframe>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgKpRQNoL9X6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "69f4d0f8-a96c-4dca-ef25-8701ea8afd79"
      },
      "source": [
        "!wget https://drive.google.com/file/d/1wroiPnT79NFRmfYW3PjWLscX9F9yoaax/view?usp=sharing\n",
        "!ls\n",
        "from IPython.display import Image\n",
        "Image('preview.1')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-13 22:52:03--  https://drive.google.com/file/d/1wroiPnT79NFRmfYW3PjWLscX9F9yoaax/view?usp=sharing\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.128.102, 142.250.128.100, 142.250.128.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.128.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘view?usp=sharing’\n",
            "\n",
            "\rview?usp=sharing        [<=>                 ]       0  --.-KB/s               \rview?usp=sharing        [ <=>                ]  71.89K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-06-13 22:52:03 (36.8 MB/s) - ‘view?usp=sharing’ saved [73613]\n",
            "\n",
            " preview   preview.1   preview.2   sample_data\t'view?usp=sharing'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b17f9de3f1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preview.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ACCEPTABLE_EMBEDDINGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot embed the '%s' image format\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot embed the '1' image format"
          ]
        }
      ]
    }
  ]
}