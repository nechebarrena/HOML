{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HOML_cap_8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRVJF+R32Cv5quxq5c/jY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nechebarrena/HOML/blob/main/HOML_cap_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLVx_Hs-Qlad"
      },
      "source": [
        "# CAPITULO 8 - HANDS ON MACHINE LEARNING\n",
        "## Reduccion de dimensionalidad\n",
        "\n",
        "Existen muchos problemas donde contamos con enormes cantidades de atributos. En general, tener muchos atributos, vuelve mas complejo al problema y uno mucha veces necesita simplificarlo. Una forma de hacer esto es reducir la cantidad de atributos que nuestro modelo va a procesar, para eso existen diversas tecnicas de reduccion de la dimensionalidad. Ademas, estas mismas tecnicas muchas veces se utilizan con el objetivo de ayudar a la visualizacion de ciertos aspectos del problema. Para estos dos objetivos el libro propone tratar las tres tecnicas princiapes, PCA, Kernel PCA, y LLE.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBN6HOICldeV"
      },
      "source": [
        "El principal problema de los sistemas con un gran numero de dimensiones es que conforme aumenta la cantidad de estas, aumenta la necesidad de datos de entrenamiento para llenar el espacio de fases. Esto se debe a que, para simplificar, al aumentar la cantidad de dimensiones aumenta el volumen del espacio y por lo tanto la cantidad de puntos que lo llenan. Esto a su vez tiene otra consecuencia un poco menos intuitiva, cuanto mas grande es la cantidad de dimensiones los puntos tienden a concentrarse mas sobre la superficie del volumen. Esto significa que en general la distancia entre 2 puntos tiende a ser mayor y por lo tanto, cuando entrenemos un modelo que haga una interpolacion entre ambos sera menos preciso. "
      ]
    }
  ]
}